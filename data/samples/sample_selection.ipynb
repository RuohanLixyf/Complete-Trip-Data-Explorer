{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Define the folder path\n",
    "# computer_villa = 'C:/Users/RuohanLi/Villanova University/Complete-trip-coordinate - General'\n",
    "# file_paths = glob.glob(computer_villa + '/Salt_Lake/delivery/Salt_Lake-Mar-2020/*.snappy.parquet')\n",
    "# df_list = [pd.read_parquet(file, engine='pyarrow') for file in file_paths]\n",
    "# # Load the first file\n",
    "# combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddd22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import pygeohash as pgh\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Folder path\n",
    "folder_path = 'C:/Users/RuohanLi/Villanova University/Complete-trip-coordinate - General'\n",
    "\n",
    "# Months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Only load needed columns\n",
    "use_cols = [\n",
    "    'linked_trip_id', 'trip_id', 'travel_mode', 'local_datetime_start', 'local_datetime_end',\n",
    "    'network_distance', 'geohash7_orig', 'geohash7_dest'\n",
    "]\n",
    "# Travel modes of interest\n",
    "modes = {'car', 'bus', 'rail', 'walk/bike'}  # use set for faster membership check\n",
    "\n",
    "# List to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "for month in months:\n",
    "    files = glob.glob(f\"{folder_path}/Salt_Lake/delivery/Salt_Lake-{month}-2020/*.snappy.parquet\")\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file, engine='pyarrow', columns=use_cols)\n",
    "        df['linked_trip_id'] = df['linked_trip_id'].astype(str)\n",
    "        df['trip_id'] = df['trip_id'].astype(str)\n",
    "        df['travel_mode'] = df['travel_mode'].astype(str).str.lower().str.strip()\n",
    "\n",
    "        # 3. 时间列转为 datetime\n",
    "        df['local_datetime_start'] = pd.to_datetime(df['local_datetime_start'], errors='coerce')\n",
    "        df['local_datetime_end'] = pd.to_datetime(df['local_datetime_end'], errors='coerce')\n",
    "\n",
    "        # 4. 过滤非法时间\n",
    "        df = df[df['local_datetime_end'] > df['local_datetime_start']]\n",
    "\n",
    "        # 5. 计算 duration\n",
    "        df['duration_minutes'] = (df['local_datetime_end'] - df['local_datetime_start']).dt.total_seconds() / 60\n",
    "\n",
    "        # 6. 清洗空间列（geohash）\n",
    "        df['geohash7_orig'] = df['geohash7_orig'].astype(str).str.strip()\n",
    "        df['geohash7_dest'] = df['geohash7_dest'].astype(str).str.strip()\n",
    "\n",
    "        # 7. 清洗距离字段（避免非数字）\n",
    "        df['network_distance'] = pd.to_numeric(df['network_distance'], errors='coerce')\n",
    "        df = df[df['network_distance'] > 0]\n",
    "        df_list.append(df)\n",
    "\n",
    "all_df = pd.concat(df_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1aada3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2. 加载地理边界并筛选Geohash对应关系 ============\n",
    "tract_path = r\"C:\\Users\\RuohanLi\\Villanova University\\Complete-trip-coordinate - General\\Manuscript\\Figure\\Visualization-RL\\2-OD patterns by census track\\six_counties_track.shp\"\n",
    "tracts = gpd.read_file(tract_path).to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Step 1: 构造 orig 和 dest 点\n",
    "gdf_orig = gpd.GeoDataFrame(\n",
    "    all_df[['geohash7_orig']],  # 只用 orig 部分\n",
    "    geometry=all_df['geohash7_orig'].apply(lambda gh: Point(pgh.decode(gh)[1], pgh.decode(gh)[0])),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_orig.index = all_df.index  # 保持 index 对齐\n",
    "\n",
    "gdf_dest = gpd.GeoDataFrame(\n",
    "    all_df[['geohash7_dest']],\n",
    "    geometry=all_df['geohash7_dest'].apply(lambda gh: Point(pgh.decode(gh)[1], pgh.decode(gh)[0])),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gdf_dest.index = all_df.index\n",
    "\n",
    "# Step 2: 空间连接 tract shapefile\n",
    "gdf_orig_joined = gpd.sjoin(gdf_orig, tracts[['GEOID', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "gdf_dest_joined = gpd.sjoin(gdf_dest, tracts[['GEOID', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 3: 把 GEOID 信息通过 index merge 回 all_df\n",
    "all_df['GEOID_orig'] = gdf_orig_joined['GEOID']\n",
    "all_df['GEOID_dest'] = gdf_dest_joined['GEOID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59e5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "\n",
    "# 加载并清洗数据：略（使用你已有的 all_df）\n",
    "\n",
    "# === 第一步：匹配 linked_trip 起点终点 ===\n",
    "sorted_df = all_df.sort_values(['linked_trip_id', 'local_datetime_start'])\n",
    "\n",
    "first_trips = sorted_df.groupby('linked_trip_id').first().reset_index()\n",
    "last_trips = sorted_df.groupby('linked_trip_id').last().reset_index()\n",
    "\n",
    "merged = first_trips[['linked_trip_id', 'GEOID_orig']] \\\n",
    "    .merge(last_trips[['linked_trip_id', 'GEOID_dest']], on='linked_trip_id')\n",
    "\n",
    "target_linked = merged[\n",
    "    (merged['GEOID_orig'] == '49035114000') &\n",
    "    (merged['GEOID_dest'] == '49035110106')\n",
    "]['linked_trip_id'].unique()\n",
    "\n",
    "target_df = sorted_df[sorted_df['linked_trip_id'].isin(target_linked)]\n",
    "\n",
    "# === 第二步：识别符合 multimodal 的 trip ===\n",
    "def is_multimodal(modes):\n",
    "    seq = list(modes)\n",
    "    mode_set = set(seq)\n",
    "    # walk-bus-walk\n",
    "    # car + (bus or rail)\n",
    "    if 'car' in mode_set and 'walk/bike' in mode_set :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filtered_trips = []\n",
    "for ltid, group in target_df.groupby('linked_trip_id'):\n",
    "    modes = list(group.sort_values('local_datetime_start')['travel_mode'])\n",
    "    # if is_multimodal(modes):\n",
    "    filtered_trips.append(ltid)\n",
    "\n",
    "sample_df = target_df[target_df['linked_trip_id'].isin(filtered_trips)]\n",
    "\n",
    "# === 第三步：生成结果表 ===\n",
    "output = []\n",
    "for ltid, group in sample_df.groupby('linked_trip_id'):\n",
    "    group = group.sort_values('local_datetime_start')\n",
    "    mode_seq = '->'.join(group['travel_mode'])\n",
    "\n",
    "    # 经纬度坐标序列\n",
    "    coords = []\n",
    "    times = []\n",
    "    for _, row in group.iterrows():\n",
    "        try:\n",
    "            o_lat, o_lng = pgh.decode(row['geohash7_orig'])\n",
    "            d_lat, d_lng = pgh.decode(row['geohash7_dest'])\n",
    "            coords.append((o_lng, o_lat))  # shapely: (x, y) = (lon, lat)\n",
    "            coords.append((d_lng, d_lat))\n",
    "            times.append(row['local_datetime_start'])\n",
    "            times.append(row['local_datetime_end'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        continue\n",
    "\n",
    "    linestring = LineString(coords)\n",
    "    times_str = '->'.join([t.strftime('%Y-%m-%d %H:%M:%S') for t in times])\n",
    "\n",
    "    output.append({\n",
    "        'linked_trip_id': ltid,\n",
    "        'mode_sequence': mode_seq,\n",
    "        'geometry_wkt': linestring.wkt,\n",
    "        'time_sequence': times_str\n",
    "    })\n",
    "\n",
    "# === 第四步：保存结果 ===\n",
    "output_df = pd.DataFrame(output)\n",
    "output_df.to_csv('multimodal_trip_sample.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f0874",
   "metadata": {},
   "source": [
    "# specific sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82bc0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. File saved as selected_trips_with_geometry.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pygeohash as pgh\n",
    "from shapely.geometry import LineString\n",
    "from shapely import wkt as shapely_wkt\n",
    "import ast\n",
    "\n",
    "# === 设置 ===\n",
    "folder_path = \"C:/Users/RuohanLi/Villanova University/Complete-trip-coordinate - General\"\n",
    "target_ids = [\n",
    "    \"KWzJGrJMlbBXGBdYWd51rRYNrlVBRNYJQLwGZylWa4lZeKq0oP0LQE-rVzWQ05jPj2X9LLBQXpAvdqLrg5QX\",  # Jan\n",
    "    \"RLaKOp9YB2WDo44DoYLLKbeKjqzoWAz9J7vM7wEJxEDBwZExB262rO-1B1XElqEwldbAnxYQlwvwAqDx1xJg\",  # Feb\n",
    "]\n",
    "\n",
    "# === 加载 Jan 和 Feb 的全部数据 ===\n",
    "months = ['Jan', 'Feb']\n",
    "use_cols = ['linked_trip_id', 'trip_id', 'geohash7_orig', 'geohash7_dest', 'route_taken', 'travel_mode']\n",
    "df_list = []\n",
    "\n",
    "for month in months:\n",
    "    files = glob.glob(os.path.join(folder_path, f\"Salt_Lake/delivery/Salt_Lake-{month}-2020/*.snappy.parquet\"))\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file, engine='pyarrow', columns=use_cols)\n",
    "        df = df[df['linked_trip_id'].isin(target_ids)]\n",
    "        df_list.append(df)\n",
    "\n",
    "target_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# === OD坐标列 ===\n",
    "def decode_geohash_pair(row):\n",
    "    try:\n",
    "        lat1, lon1 = pgh.decode(row['geohash7_orig'])\n",
    "        lat2, lon2 = pgh.decode(row['geohash7_dest'])\n",
    "        return pd.Series([lon1, lat1, lon2, lat2])\n",
    "    except:\n",
    "        return pd.Series([None, None, None, None])\n",
    "\n",
    "target_df[['orig_lon', 'orig_lat', 'dest_lon', 'dest_lat']] = target_df.apply(decode_geohash_pair, axis=1)\n",
    "\n",
    "# === 加载路网并构建 mode-specific 字典 ===\n",
    "link_car = pd.read_csv(os.path.join(folder_path, \"Salt_Lake/supplementInputs/network/auto-biggest-connected-graph/link.csv\"))\n",
    "link_car_dict = {\n",
    "    (int(row['from_osm_node_id']), int(row['to_osm_node_id'])): row['geometry']\n",
    "    for _, row in link_car.iterrows()\n",
    "}\n",
    "\n",
    "link_transit = pd.read_csv(os.path.join(folder_path, \"Salt_Lake/supplementInputs/network/UTA/link with flow.csv\"))\n",
    "link_transit_dict = {\n",
    "    (int(row['from_node_id']), int(row['to_node_id'])): row['geometry']\n",
    "    for _, row in link_transit.iterrows()\n",
    "}\n",
    "\n",
    "# === 构造 trip-level 汇总结果 ===\n",
    "output_rows = []\n",
    "for ltid, group in target_df.groupby('linked_trip_id'):\n",
    "    all_geoms = []\n",
    "    for _, row in group.iterrows():\n",
    "        try:\n",
    "            # 解析节点序列\n",
    "            nodes = [int(n.strip()) for n in row['route_taken'].split(',') if n.strip().isdigit() and int(n.strip()) != -1]\n",
    "            if len(nodes) < 2:\n",
    "                continue\n",
    "\n",
    "            # 根据 mode 选取 link_dict\n",
    "            mode = row['travel_mode'].lower()\n",
    "            if mode == 'car':\n",
    "                link_dict = link_car_dict\n",
    "            elif mode in ['bus', 'rail']:\n",
    "                link_dict = link_transit_dict\n",
    "            else:\n",
    "                continue  # skip walk/bike/air etc.\n",
    "\n",
    "            # 查找 link geometry\n",
    "            for i in range(len(nodes)-1):\n",
    "                pair = (nodes[i], nodes[i+1])\n",
    "                if pair in link_dict:\n",
    "                    try:\n",
    "                        linestring = shapely_wkt.loads(link_dict[pair])\n",
    "                        all_geoms.extend(list(linestring.coords))\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ WKT parse error for pair {pair}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in trip {row['trip_id']}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if all_geoms:\n",
    "        full_linestring = LineString(all_geoms)\n",
    "        output_rows.append({\n",
    "            'linked_trip_id': ltid,\n",
    "            'trip_count': len(group),\n",
    "            'full_geometry_wkt': full_linestring.wkt\n",
    "        })\n",
    "\n",
    "# === 合并结果并保存 ===\n",
    "if output_rows:\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    target_df = target_df.merge(output_df, on='linked_trip_id', how='left')\n",
    "else:\n",
    "    print(\"⚠️ No link geometries were matched. Adding empty column.\")\n",
    "    target_df['full_geometry_wkt'] = None\n",
    "\n",
    "target_df.to_csv(\"selected_trips_with_geometry.csv\", index=False)\n",
    "print(\"✅ Done. File saved as selected_trips_with_geometry.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fadbe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to selected_linked_trips_jan_feb.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 设置路径和 ID\n",
    "folder_path = \"C:/Users/RuohanLi/Villanova University/Complete-trip-coordinate - General\"\n",
    "target_ids = [\n",
    "    \"KWzJGrJMlbBXGBdYWd51rRYNrlVBRNYJQLwGZylWa4lZeKq0oP0LQE-rVzWQ05jPj2X9LLBQXpAvdqLrg5QX\",  # Jan\n",
    "    \"RLaKOp9YB2WDo44DoYLLKbeKjqzoWAz9J7vM7wEJxEDBwZExB262rO-1B1XElqEwldbAnxYQlwvwAqDx1xJg\",  # Feb\n",
    "]\n",
    "months = ['Jan', 'Feb']\n",
    "\n",
    "# 读取并筛选数据\n",
    "df_list = []\n",
    "for month in months:\n",
    "    files = glob.glob(os.path.join(folder_path, f\"Salt_Lake/delivery/Salt_Lake-{month}-2020/*.snappy.parquet\"))\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        filtered = df[df['linked_trip_id'].isin(target_ids)]\n",
    "        if not filtered.empty:\n",
    "            df_list.append(filtered)\n",
    "\n",
    "# 合并并保存\n",
    "if df_list:\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "    result_df.to_csv(\"selected_linked_trips_jan_feb.csv\", index=False)\n",
    "    print(\"✅ Saved to selected_linked_trips_jan_feb.csv\")\n",
    "else:\n",
    "    print(\"⚠️ No matching linked_trip_id found in Jan or Feb.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
